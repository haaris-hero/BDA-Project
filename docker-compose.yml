version: "3.8"

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - food-delivery-net
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181

      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_RETENTION_HOURS: 2
    ports:
      - "9092:9092"
      - "29092:29092"
    networks:
      - food-delivery-net
    restart: unless-stopped

  mongo:
    image: mongo:4.4
    container_name: mongo
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: password123
      MONGO_INITDB_DATABASE: food_delivery
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
      - ./scripts/mongo_init.js:/docker-entrypoint-initdb.d/init.js:ro
    networks:
      - food-delivery-net
    restart: unless-stopped
    mem_limit: 1g
    command: mongod

  hadoop-namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    environment:
      CLUSTER_NAME: food_delivery_cluster
      CORE_CONF_fs_defaultFS: hdfs://hadoop-namenode:8020
      CORE_CONF_hadoop_http_staticuser_user_name: root
    ports:
      - "50070:9870"
      - "8020:8020"
    volumes:
      - namenode_data:/hadoop/dfs/name
    networks:
      - food-delivery-net
    restart: unless-stopped

  hadoop-datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    depends_on:
      - hadoop-namenode
    environment:
      SERVICE_PRECONDITION: hadoop-namenode:8020
      CORE_CONF_fs_defaultFS: hdfs://hadoop-namenode:8020
    ports:
      - "50075:50075"
    volumes:
      - datanode_data:/hadoop/dfs/data
    networks:
      - food-delivery-net
    restart: unless-stopped

  spark-master:
    image: apache/spark:latest
    container_name: spark-master
    environment:
      SPARK_MODE: master
    ports:
      - "8080:8080"
      - "7077:7077"
      - "6066:6066"
    volumes:
      - ./spark_jobs:/opt/spark-apps
    networks:
      - food-delivery-net
    restart: unless-stopped
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master

  spark-worker-1:
    image: apache/spark:latest
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    ports:
      - "8081:8081"
    volumes:
      - ./spark_jobs:/opt/spark-apps
    networks:
      - food-delivery-net
    restart: unless-stopped
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  spark-worker-2:
    image: apache/spark:latest
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 1G
      SPARK_WORKER_CORES: 1
    ports:
      - "8082:8081"
    volumes:
      - ./spark_jobs:/opt/spark-apps
    networks:
      - food-delivery-net
    restart: unless-stopped
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077

  orders-generator:
    build:
      context: .
      dockerfile: Dockerfile.generator
    command: python /app/scripts/generate_orders_stream.py
    depends_on:
      - kafka
    networks:
      - food-delivery-net
    restart: unless-stopped

  riders-generator:
    build:
      context: .
      dockerfile: Dockerfile.generator
    command: python /app/scripts/generate_rider_stream.py
    depends_on:
      - kafka
    networks:
      - food-delivery-net
    restart: unless-stopped

  kitchen-generator:
    build:
      context: .
      dockerfile: Dockerfile.generator
    command: python /app/scripts/generate_kitchen_stream.py
    depends_on:
      - kafka
    networks:
      - food-delivery-net
    restart: unless-stopped

  airflow-postgres:
    image: postgres:13-alpine
    container_name: airflow-postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - food-delivery-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      timeout: 5s
      retries: 5

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-webserver
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: "81HqDtbqAywKSOumSha3BqDTh0yrLiIkVlAJFQDm0Q="
      AIRFLOW__CORE__DAGS_FOLDER: /home/airflow/gcs/dags
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW_HOME: /home/airflow
    ports:
      - "8888:8080"
    volumes:
      - ./dags:/home/airflow/gcs/dags
      - ./logs:/home/airflow/logs
      - ./plugins:/home/airflow/plugins
      - ./scripts:/opt/airflow/scripts
      - ./spark_jobs:/opt/spark-apps
      - ./spark:/opt/spark
    group_add:
      - "989"   # docker group id
    command: >
      bash -c "
      airflow db upgrade &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
      airflow webserver --port 8080
      "
    networks:
      - food-delivery-net
    restart: unless-stopped

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow-scheduler
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__FERNET_KEY: "81HqDtbqAywKSOumSha3BqDTh0yrLiIkVlAJFQDm0Q="
      AIRFLOW__CORE__DAGS_FOLDER: /home/airflow/gcs/dags
      AIRFLOW_HOME: /home/airflow
      DOCKER_API_VERSION: "1.44"
    volumes:
      - ./dags:/home/airflow/gcs/dags
      - ./logs:/home/airflow/logs
      - ./plugins:/home/airflow/plugins
      - ./spark_jobs:/opt/airflow/spark_jobs
      - ./scripts:/opt/airflow/scripts
      - /var/run/docker.sock:/var/run/docker.sock
      - ./spark_jobs:/opt/spark-apps
      - ./spark:/opt/spark
    group_add:
      - "989"   # docker group id
    command: airflow scheduler
    networks:
      - food-delivery-net
    restart: unless-stopped

  kafka-ingestion:
    build:
      context: .
      dockerfile: Dockerfile.ingestion
    volumes:
      - ./scripts:/opt/airflow/scripts
    command: python /opt/airflow/scripts/ingest_kafka_to_mongo.py
    depends_on:
      - kafka
      - mongo
    networks:
      - food-delivery-net
    restart: always

  streamlit-dashboard:
    build: ./dashboard
    container_name: streamlit-dashboard
    ports:
      - "8501:8501"
    depends_on:
      - mongo
    networks:
      - food-delivery-net
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - food-delivery-net
    restart: unless-stopped
    command: redis-server --appendonly yes

volumes:
  mongo_data:
  namenode_data:
  datanode_data:
  postgres_data:
  redis_data:

networks:
  food-delivery-net:
    driver: bridge
